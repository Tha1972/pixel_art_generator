{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84914703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from noise_scheduler import LinearNoiseScheduler\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eedf39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Time embeddings\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, 256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, 256),\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.GroupNorm(8, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.GroupNorm(8, 64),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(64, 64, 4, stride=2, padding=1)   # 16 → 8\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.GroupNorm(8, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.GroupNorm(8, 128),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(128, 128, 4, stride=2, padding=1) # 8 → 4\n",
    "\n",
    "        # Bottleneck\n",
    "        self.mid = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.GroupNorm(8, 256),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.GroupNorm(8, 256),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode=\"nearest\")   # 4 → 8\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(256 + 128, 128, 3, padding=1),\n",
    "            nn.GroupNorm(8, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.GroupNorm(8, 128),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode=\"nearest\")   # 8 → 16\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(128 + 64, 64, 3, padding=1),\n",
    "            nn.GroupNorm(8, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.GroupNorm(8, 64),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # Output\n",
    "        self.out = nn.Conv2d(64, 3, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Timestep embedding\n",
    "        t = t.float().unsqueeze(1)\n",
    "        t_emb = self.time_mlp(t)\n",
    "\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e1 = e1 + t_emb[:, :64, None, None]\n",
    "\n",
    "        x = self.down1(e1)\n",
    "\n",
    "        e2 = self.enc2(x)\n",
    "        e2 = e2 + t_emb[:, :128, None, None]\n",
    "\n",
    "        x = self.down2(e2)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.mid(x)\n",
    "        x = x + t_emb[:, :256, None, None]\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x, e2], dim=1)\n",
    "        x = self.dec1(x)\n",
    "        x = x + t_emb[:, :128, None, None]\n",
    "\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, e1], dim=1)\n",
    "        x = self.dec2(x)\n",
    "        x = x + t_emb[:, :64, None, None]\n",
    "\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d30af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab46a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"data/images/images\"\n",
    "image_paths = [\n",
    "    os.path.join(image_dir, f).split(\"/\")[3].split(\".\")[0]\n",
    "    for f in os.listdir(image_dir)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be2ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_label_paths = df[df[\"Label\"] == \"[1. 0. 0. 0. 0.]\"][\"Image Path\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0add7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_paths = [i.split(\"/\")[2].split(\".\")[0] for i in first_label_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db22e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "            os.path.join(image_dir, f).split(\"/\")[3].split(\".\")[0]\n",
    "            for f in os.listdir(image_dir)\n",
    "            if os.path.join(image_dir, f).split(\"/\")[3].split(\".\")[0] in filtered_paths\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e9426a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef8d2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_paths = [\n",
    "            os.path.join(image_dir, f)\n",
    "            for f in os.listdir(image_dir)\n",
    "            if os.path.join(image_dir, f).split(\"/\")[3].split(\".\")[0] in filtered_paths\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "        img = img * 2 - 1         \n",
    "\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626f420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample(x0, t, scheduler):\n",
    "    noise = torch.randn_like(x0)\n",
    "\n",
    "    sqrt_ab = scheduler.sqrt_alpha_cum_prod[t][:, None, None, None]\n",
    "    sqrt_omab = scheduler.sqrt_one_minus_alpha_cum_prod[t][:, None, None, None]\n",
    "\n",
    "    x_t = sqrt_ab * x0 + sqrt_omab * noise\n",
    "    return x_t, noise\n",
    "\n",
    "def diffusion_loss(model, x0, scheduler):\n",
    "    B = x0.size(0)\n",
    "    device = x0.device\n",
    "\n",
    "    t = torch.randint(0, scheduler.num_timesteps, (B,), device=device)\n",
    "\n",
    "    x_t, noise = q_sample(x0, t, scheduler)\n",
    "    pred_noise = model(x_t, t)\n",
    "\n",
    "    return F.mse_loss(pred_noise, noise)\n",
    "\n",
    "def train_diffusion(\n",
    "    model,\n",
    "    dataloader,\n",
    "    epochs,\n",
    "    lr=1e-4,\n",
    "    num_timesteps=100,\n",
    "    device=\"mps\"\n",
    "):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = LinearNoiseScheduler(\n",
    "        num_timesteps=num_timesteps,\n",
    "        beta_start=1e-4,\n",
    "        beta_end=0.02\n",
    "        )\n",
    "    \n",
    "    scheduler.betas = scheduler.betas.to(device)\n",
    "    scheduler.alphas = scheduler.alphas.to(device)\n",
    "    scheduler.alpha_cum_prod = scheduler.alpha_cum_prod.to(device)\n",
    "    scheduler.sqrt_alpha_cum_prod = scheduler.sqrt_alpha_cum_prod.to(device)\n",
    "    scheduler.sqrt_one_minus_alpha_cum_prod = scheduler.sqrt_one_minus_alpha_cum_prod.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i, images in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "\n",
    "            loss = diffusion_loss(model, images, scheduler)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train\n",
      "Epoch [1/50] | Loss: 1.413956\n",
      "Epoch [2/50] | Loss: 0.556412\n",
      "Epoch [3/50] | Loss: 0.316219\n",
      "Epoch [4/50] | Loss: 0.266237\n",
      "Epoch [5/50] | Loss: 0.237790\n",
      "Epoch [6/50] | Loss: 0.216530\n",
      "Epoch [7/50] | Loss: 0.197195\n",
      "Epoch [8/50] | Loss: 0.182830\n",
      "Epoch [9/50] | Loss: 0.171435\n",
      "Epoch [10/50] | Loss: 0.154507\n",
      "Epoch [11/50] | Loss: 0.149378\n",
      "Epoch [12/50] | Loss: 0.134369\n",
      "Epoch [13/50] | Loss: 0.126591\n",
      "Epoch [14/50] | Loss: 0.118167\n",
      "Epoch [15/50] | Loss: 0.111472\n",
      "Epoch [16/50] | Loss: 0.104754\n",
      "Epoch [17/50] | Loss: 0.100694\n",
      "Epoch [18/50] | Loss: 0.091670\n",
      "Epoch [19/50] | Loss: 0.089166\n",
      "Epoch [20/50] | Loss: 0.086923\n",
      "Epoch [21/50] | Loss: 0.084750\n",
      "Epoch [22/50] | Loss: 0.080305\n",
      "Epoch [23/50] | Loss: 0.078105\n",
      "Epoch [24/50] | Loss: 0.073679\n",
      "Epoch [25/50] | Loss: 0.072449\n",
      "Epoch [26/50] | Loss: 0.070267\n",
      "Epoch [27/50] | Loss: 0.067304\n",
      "Epoch [28/50] | Loss: 0.064411\n",
      "Epoch [29/50] | Loss: 0.065358\n",
      "Epoch [30/50] | Loss: 0.061521\n",
      "Epoch [31/50] | Loss: 0.062645\n",
      "Epoch [32/50] | Loss: 0.060817\n",
      "Epoch [33/50] | Loss: 0.056545\n",
      "Epoch [34/50] | Loss: 0.057123\n",
      "Epoch [35/50] | Loss: 0.057206\n",
      "Epoch [36/50] | Loss: 0.055938\n",
      "Epoch [37/50] | Loss: 0.053443\n",
      "Epoch [38/50] | Loss: 0.052955\n",
      "Epoch [39/50] | Loss: 0.052410\n",
      "Epoch [40/50] | Loss: 0.051784\n",
      "Epoch [41/50] | Loss: 0.049146\n",
      "Epoch [42/50] | Loss: 0.048314\n",
      "Epoch [43/50] | Loss: 0.047485\n",
      "Epoch [44/50] | Loss: 0.050594\n",
      "Epoch [45/50] | Loss: 0.047312\n",
      "Epoch [46/50] | Loss: 0.045553\n",
      "Epoch [47/50] | Loss: 0.046308\n",
      "Epoch [48/50] | Loss: 0.043932\n",
      "Epoch [49/50] | Loss: 0.045971\n",
      "Epoch [50/50] | Loss: 0.041835\n"
     ]
    }
   ],
   "source": [
    "model = UNet()\n",
    "\n",
    "dataset = ImageFolderDataset(\"data/images/images\")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(\"Starting train\")\n",
    "train_diffusion(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    epochs=50,\n",
    "    lr=1e-4,\n",
    "    num_timesteps=75,\n",
    "    device=\"mps\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b08c51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(\n",
    "    model,\n",
    "    scheduler,\n",
    "    image_size=16,\n",
    "    device=\"mps\"\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.randn(1, 3, image_size, image_size, device=device)\n",
    "\n",
    "    for t in reversed(range(scheduler.num_timesteps)):\n",
    "        t_tensor = torch.tensor([t], device=device)\n",
    "\n",
    "        pred_noise = model(x, t_tensor)\n",
    "\n",
    "        alpha = scheduler.alphas[t]\n",
    "        alpha_bar = scheduler.alpha_cum_prod[t]\n",
    "        beta = scheduler.betas[t]\n",
    "\n",
    "        x = (1 / torch.sqrt(alpha)) * (\n",
    "            x - (beta / torch.sqrt(1 - alpha_bar)) * pred_noise\n",
    "        )\n",
    "\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            x = x + torch.sqrt(beta) * noise\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1402fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LinearNoiseScheduler(\n",
    "    num_timesteps=100,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2e68d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"  # or cuda\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "sampled = sample(model, scheduler, image_size=16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "85caedd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAACt9JREFUeJzt20uIHXYZxuH/zJyZzC1j0qa3pElrk1bTWLVeS9WFUBWt2kWFihsLokIpuHHhhaIuXHiBKlYXRVAUQSxItdUqiCCttqUYqbFt0sYQ0yT2Mskkk7meuRzpoq8LF54KH7PweXaB8DLMTP6/8y0y0Ov1eg0AWmuDvgsAvEwUAAhRACBEAYAQBQBCFAAIUQAgRAGA6LQ+fef2L7Yqb339lWXbD/3hd2XbL04/X7Z9xSXbW6VtO3aWbR858nTZ9vxzR8u2Zwfnyra7Y3X/R/Tzt325bPuPjxwo2777R/e0Ss/OzZZtL4zW/Ty7A3Xbzz/7398slwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgBEp/Xp2qt3tirPHPpL2falF20t294xPlq2/bq9b2yVNp83VLa9e0vdZ43BoavKtu//7QNl2x98/wfLtn/145+XbS9P9sq2l0aWWqXJ8yfKts/MLpZtrw6st43kUgAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRAEAUAPhPLgUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGA6LQ+HT15vFV5bmaubHtrb7Vs+93veFvZ9sH9j7VK2zddWLZ98PEjZdtD42XT7e1vuK5s+4n9z5RtT46OlG1fsntn2fZbTp1rlc7bs6ts+/s/+3XZdmdwqW0klwIAIQoAhCgAEKIAQIgCACEKAIQoACAKAPwnlwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoARKf1aduuHa3K+HjfX8YrduXFW8q2Dzz8aNn21PpEqzQ6Plq2vX3bxWXb0/Nnyra70+tl2zt37CnbPnTyeNn2+nTd93upt9oqPfX3Z8u2d155adn2kb8fbhvJpQBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgDRaX2643tfbVXu/fq3yraP73+ibHtkdKRse2psc6t08p8zZdurw3Vf++Sey8q257rLZdsrE5Nl2+3C2bLp1bX1su3Bob6fn//JkcPHyrafmzlTtt0bXGsbyaUAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoARKf1aXWllRkYHS/bHp4cK9vec8Xesu0nnz7cKk1smyzbPj7XLdt+9PHHyrYv3be7bPtN2+u2r3nTrrLt39x7X9n2wweeaJXmustl2wODdW/WwMhw20guBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAIhO69OZU2utyuT4SNn23Nn5su2Fc72y7cu272qV7j/4aNn2ke5w2fb9x/9ctn3zda8t2778wm1l2/NrM2Xbv/zT/rLtpe7mVmn57ErZ9srm5bLt1ptoG8mlAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCANFpfeoudVuVU6fPlm0vLq+XbQ9tGinbnl5dbpUu2LG3bPv0yHDZ9uLBx8u2V3dOlG3PrC2Wbf/ka98o2z41e6Zse7DwTXnJ2NB42fbmqbGy7bnlhbaRXAoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQndanrStTrcrawkDZ9sLQStn25vGxsu1zZ2p7fXq9bntpaLhs+803vKdse9/u15Rtf/uOz5Zt7xi5pGx7ZLHvJ+IVe9XmujflJacWzpZtry/V/ftcWJptG8mlAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCANFpfepOLbQqK5uGyrY7W6fKto+dmCnb3rLnmlZp79T2su1Xj06Ubd+4a7xs+3O331q2/ZmPfKJs++67vl+2PTnW9xPxil205YJWaXhsU9n2wReOlW0Pr/faRnIpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQHRan0bGp1qVhfGBsu2nXvhH2fb7PnxL2faR6XOt0rnufNn2wtyZsu1zB+o+x9z/3XvKtj/6oQ+UbX/lU58u257+5+my7R/8pu77/ZLnFxfLtjeN9v10vmID/T/LJVwKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEJ3Wp5nufKuyPLBatn39jR8q237kyf1l2ytjo63SfLdu+/LzryjbHuoOlW0PdM+Vbf/ivp+Wbd/9zbvKtq9955vLti9/4spW6YWjh8q2JzaPl23PPHeibSSXAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAESn9ak3OdKqHJs5WbZ99dV7y7bXttY19ejxp1ulG65/f9n2iedPlG2PtImy7aXhbWXbbW2gbPqa976tbPtrd36vbPtjt3ysVXrwzr+WbS+ud8u2V0Y3tY3kUgAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBioNfr9VofLt59Wauysjxbtv3Nu75Utr1rzyVl2/Pzc63SIw8/XLb9zFOHy7b3vXpf2fb1b3lX2fZtt36hbHvq/E7Z9tzpU2Xbuy67ulU6cPCZsu31sfGy7eHectn2yeMn/uvfcSkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAdFqfhmbPtSqDYwNl27suHC/b/sHd3yjbHu31/aP5n9z03pvLtvdtuahs+6EHHy/b/v302bLtHz7wnbLtj9/0ybLtbuHv4dEjh1ul88a2lm2fXpor2+5tqnsP++FSACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgOi0Po1PTLYqM2vTZdtzi8cKt9fKtoc6vVape/rFsu3XXHFV2fbZ+dmy7b8dOlS2Pbi6ULY9t7Jctt0ZHirbHh2s/Uw6fXa+bLvTWS/b7q5u7Gd1lwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDEQK/X6/37jwD8P3MpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgA0F72L17umLxCuMY7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = sampled.squeeze(0)\n",
    "img = (img + 1) / 2         \n",
    "img = img.clamp(0, 1)\n",
    "img = img.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c837d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2ee9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixel_art_generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
